{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9975ffc",
   "metadata": {},
   "source": [
    "evaluation using prompt into claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec004918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jahna\\AppData\\Local\\Temp\\ipykernel_5224\\2192669085.py:35: DeprecationWarning: The model 'claude-3-5-sonnet-20240620' is deprecated and will reach end-of-life on October 22, 2025.\n",
      "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
      "  response = client.messages.create(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude's Raw Output: Here's my evaluation of the RAG response:\n",
      "\n",
      "{\n",
      "  \"faithfulness\": 5,\n",
      "  \"relevance\": 5,\n",
      "  \"completeness\": 5,\n",
      "  \"clarity\": 5,\n",
      "  \"comments\": \"The answer is perfectly faithful to the context, directly relevant to the query, complete in addressing the question, and clear in its concise statement. It provides the exact information requested without embellishment or omission.\"\n",
      "}\n",
      "Could not parse JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "import json\n",
    "\n",
    "# Initialize Claude client\n",
    "client = Anthropic()\n",
    "\n",
    "query = \"What is the capital of France?\"\n",
    "retrieved_context = \"France is a country in Europe. Its capital city is Paris.\"\n",
    "rag_response = \"The capital of France is Paris.\"\n",
    "\n",
    "# evaluation prompt\n",
    "prompt = f\"\"\"\n",
    "You are an evaluator. Rate the following RAG response based on four criteria:\n",
    "1. Faithfulness: Does it stick to the retrieved context without hallucinating?\n",
    "2. Relevance: Does it answer the user query?\n",
    "3. Completeness: Does it cover all necessary aspects of the query?\n",
    "4. Clarity: Is the answer easy to understand?\n",
    "\n",
    "Query: {query}\n",
    "Retrieved Context: {retrieved_context}\n",
    "Generated Answer: {rag_response}\n",
    "\n",
    "Return your evaluation as a JSON object like this:\n",
    "{{\n",
    "  \"faithfulness\": 1-5,\n",
    "  \"relevance\": 1-5,\n",
    "  \"completeness\": 1-5,\n",
    "  \"clarity\": 1-5,\n",
    "  \"comments\": \"short explanation\"\n",
    "}}\n",
    "\"\"\"\n",
    "# claude's evaluation\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",  \n",
    "    max_tokens=300,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    ")\n",
    "\n",
    "eval_text = response.content[0].text\n",
    "print(\"Claude's Raw Output:\", eval_text)\n",
    "\n",
    "try:\n",
    "    eval_json = json.loads(eval_text)\n",
    "    print(\"Parsed Evaluation:\", json.dumps(eval_json, indent=2))\n",
    "except Exception as e:\n",
    "    print(\"Could not parse JSON:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106e343",
   "metadata": {},
   "source": [
    "through bleu and rouge scores (better if higher) (with respect to refernce answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ef910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 8.38826642100846e-155\n",
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=0.6666666666666666, recall=0.6666666666666666, fmeasure=0.6666666666666666)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Python312\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "reference = \"The capital of France is Paris.\"\n",
    "generated = rag_response\n",
    "\n",
    "# bleu score\n",
    "bleu = sentence_bleu([reference.split()], generated.split())\n",
    "print(\"BLEU Score:\", bleu)\n",
    "\n",
    "# rouge score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rougeL'], use_stemmer=True)\n",
    "rouge_scores = scorer.score(reference, generated)\n",
    "print(\"ROUGE Scores:\", rouge_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ed469e",
   "metadata": {},
   "source": [
    "check for hallucination (if value is low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4311c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribution Overlap Ratio: 0.4090909090909091\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "ratio = SequenceMatcher(None, retrieved_context, rag_response).ratio()\n",
    "print(\"Attribution Overlap Ratio:\", ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f9a61",
   "metadata": {},
   "source": [
    "other methods which could be used:\n",
    "- semantic check: with respect to reference answer if it's is given\n",
    "- fluency check: how natural the response is (through pretrained transformers like gpt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
