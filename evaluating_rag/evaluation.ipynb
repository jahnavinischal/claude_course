{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9975ffc",
   "metadata": {},
   "source": [
    "evaluation using prompt into claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec004918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jahna\\AppData\\Local\\Temp\\ipykernel_5224\\2192669085.py:35: DeprecationWarning: The model 'claude-3-5-sonnet-20240620' is deprecated and will reach end-of-life on October 22, 2025.\n",
      "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
      "  response = client.messages.create(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude's Raw Output: Here's my evaluation of the RAG response:\n",
      "\n",
      "{\n",
      "  \"faithfulness\": 5,\n",
      "  \"relevance\": 5,\n",
      "  \"completeness\": 5,\n",
      "  \"clarity\": 5,\n",
      "  \"comments\": \"The answer is perfectly faithful to the context, directly relevant to the query, complete in addressing the question, and clear in its concise statement. It provides the exact information requested without embellishment or omission.\"\n",
      "}\n",
      "Could not parse JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "import json\n",
    "\n",
    "# Initialize Claude client\n",
    "client = Anthropic()\n",
    "\n",
    "query = \"What is the capital of France?\"\n",
    "retrieved_context = \"France is a country in Europe. Its capital city is Paris.\"\n",
    "rag_response = \"The capital of France is Paris.\"\n",
    "\n",
    "# evaluation prompt\n",
    "prompt = f\"\"\"\n",
    "You are an evaluator. Rate the following RAG response based on four criteria:\n",
    "1. Faithfulness: Does it stick to the retrieved context without hallucinating?\n",
    "2. Relevance: Does it answer the user query?\n",
    "3. Completeness: Does it cover all necessary aspects of the query?\n",
    "4. Clarity: Is the answer easy to understand?\n",
    "\n",
    "Query: {query}\n",
    "Retrieved Context: {retrieved_context}\n",
    "Generated Answer: {rag_response}\n",
    "\n",
    "Return your evaluation as a JSON object like this:\n",
    "{{\n",
    "  \"faithfulness\": 1-5,\n",
    "  \"relevance\": 1-5,\n",
    "  \"completeness\": 1-5,\n",
    "  \"clarity\": 1-5,\n",
    "  \"comments\": \"short explanation\"\n",
    "}}\n",
    "\"\"\"\n",
    "# claude's evaluation\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",  \n",
    "    max_tokens=300,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    ")\n",
    "\n",
    "eval_text = response.content[0].text\n",
    "print(\"Claude's Raw Output:\", eval_text)\n",
    "\n",
    "try:\n",
    "    eval_json = json.loads(eval_text)\n",
    "    print(\"Parsed Evaluation:\", json.dumps(eval_json, indent=2))\n",
    "except Exception as e:\n",
    "    print(\"Could not parse JSON:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106e343",
   "metadata": {},
   "source": [
    "through bleu and rouge scores (better if higher) (with respect to refernce answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ef910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 8.38826642100846e-155\n",
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=0.6666666666666666, recall=0.6666666666666666, fmeasure=0.6666666666666666)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Python312\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "reference = \"The capital of France is Paris.\"\n",
    "generated = rag_response\n",
    "\n",
    "# bleu score\n",
    "bleu = sentence_bleu([reference.split()], generated.split())\n",
    "print(\"BLEU Score:\", bleu)\n",
    "\n",
    "# rouge score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rougeL'], use_stemmer=True)\n",
    "rouge_scores = scorer.score(reference, generated)\n",
    "print(\"ROUGE Scores:\", rouge_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ed469e",
   "metadata": {},
   "source": [
    "check for hallucination (if value is low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4311c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribution Overlap Ratio: 0.4090909090909091\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "ratio = SequenceMatcher(None, retrieved_context, rag_response).ratio()\n",
    "print(\"Attribution Overlap Ratio:\", ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f9a61",
   "metadata": {},
   "source": [
    "other methods which could be used:\n",
    "- semantic check: with respect to reference answer if it's is given\n",
    "- fluency check: how natural the response is (through pretrained transformers like gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62ca060",
   "metadata": {},
   "source": [
    "# updated code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442fc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"faithfulness\": 0.904,\n",
      "  \"relevance\": 0.879,\n",
      "  \"fluency\": 0.224,\n",
      "  \"final_score\": 0.758\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Initialize models \n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Example RAG data (can be replaced dynamically)\n",
    "query = \"What is the capital of France?\"\n",
    "retrieved_context = \"France is a country in Europe. Its capital city is Paris.\"\n",
    "rag_response = \"The capital of France is Paris.\"\n",
    "\n",
    "# can be replaced by this while evaluating on a given RAG data:\n",
    "\n",
    "# rag_data = {\n",
    "#     \"query\": user_input,\n",
    "#     \"context\": retrieved_docs,\n",
    "#     \"response\": model_output\n",
    "# }\n",
    "# result = evaluate_rag_response(rag_data)\n",
    "\n",
    "\n",
    "# Semantic Similarity Metrics \n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    emb_a = embed_model.encode(a, convert_to_tensor=True)\n",
    "    emb_b = embed_model.encode(b, convert_to_tensor=True)\n",
    "    return util.cos_sim(emb_a, emb_b).item()\n",
    "\n",
    "faithfulness = cosine_sim(rag_response, retrieved_context)\n",
    "relevance = cosine_sim(rag_response, query)\n",
    "\n",
    "# Fluency / Perplexity Metric \n",
    "def compute_perplexity(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        loss = gpt2_model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
    "    return torch.exp(loss).item()\n",
    "\n",
    "perplexity = compute_perplexity(rag_response)\n",
    "fluency = 1 / perplexity  # invert since lower perplexity = better\n",
    "\n",
    "# Normalize & Combine \n",
    "\n",
    "# Normalize fluency roughly between 0-1\n",
    "fluency_score = min(1.0, fluency * 10)\n",
    "\n",
    "# Weighted composite score \n",
    "final_score = round((0.4 * faithfulness + 0.4 * relevance + 0.2 * fluency_score), 3)\n",
    "\n",
    "# Output \n",
    "\n",
    "evaluation = {\n",
    "    \"faithfulness\": round(faithfulness, 3),\n",
    "    \"relevance\": round(relevance, 3),\n",
    "    \"fluency\": round(fluency_score, 3),\n",
    "    \"final_score\": final_score,\n",
    "}\n",
    "\n",
    "print(json.dumps(evaluation, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49e522",
   "metadata": {},
   "source": [
    "-> these scores are semantic and reference-free \n",
    "\n",
    "-> higher = better response quality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb17bbd",
   "metadata": {},
   "source": [
    "for input based evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0582482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG Response Evaluator ===\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the sentence embedding model (runs fine on CPU)\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def evaluate_rag_response(query, context, response):\n",
    "    # Encode all inputs\n",
    "    q_emb = model.encode(query, convert_to_tensor=True)\n",
    "    c_emb = model.encode(context, convert_to_tensor=True)\n",
    "    r_emb = model.encode(response, convert_to_tensor=True)\n",
    "\n",
    "    # Compute semantic similarity\n",
    "    faithfulness = util.cos_sim(r_emb, c_emb).item()  # response aligns with context\n",
    "    relevance = util.cos_sim(r_emb, q_emb).item()     # response answers query\n",
    "\n",
    "    # Weighted score\n",
    "    final_score = round((faithfulness * 0.6 + relevance * 0.4), 3)\n",
    "\n",
    "    return {\n",
    "        \"faithfulness\": round(faithfulness, 3),\n",
    "        \"relevance\": round(relevance, 3),\n",
    "        \"final_score\": final_score,\n",
    "        \"comments\": \"Higher score = better response quality. Semantic, not keyword-based.\"\n",
    "    }\n",
    "\n",
    "# ðŸ§  Interactive Input Section\n",
    "print(\"=== RAG Response Evaluator ===\")\n",
    "query = input(\"\\nEnter the user query: \")\n",
    "context = input(\"Enter the retrieved context: \")\n",
    "response = input(\"Enter the model-generated response: \")\n",
    "\n",
    "# Evaluate\n",
    "result = evaluate_rag_response(query, context, response)\n",
    "print(\"\\n--- Evaluation Result ---\")\n",
    "for k, v in result.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
